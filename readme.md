一个从第一版主采集小说的爬虫 嘿嘿嘿

2020.7.30

   以前第一版主的文字图片命名较为规律，所以采取的方式想看那本书就用爬虫去采集那本的策略。最近网站更新了策略，文字图片命名没有规律了，继续这样搞的话就很吃力，所以转化策略，想把整个网站的网页都扒下来在再慢慢解析，目前扫描出来共用30多万章节数据，因为php只能单线程，所以爬的很慢，爬虫跑了3天目前只爬取了9万条数据下来。

说说这次的爬取思路： 
    第一版主采用文字中插入图片的方法来防止爬虫爬取，而且会定期更改图片资源的引用链接。所有这次我的大致思路是先把所有的网页爬下来存到数据库中，然后对这些数据进行扫描抓出图片，再把图片资源爬取下来，最后手动识别图片资源中的文字(因为图片资源用的是矢量图,没法用ocr提取文字，这点还是比较烦的),最后再对资源扫描，根据上一步弄好的图片字典,替换掉图片，这样就完成了解析。

具体流程 
    1.扫描书籍信息 
    先从网站中抓取到所有书籍的信息 
    2.扫描书籍章节信息 
    根据上一步抓取的书籍信息，抓取书籍的章节信息 
    3.抓取章节内容 
    根据上一步抓取的章节信息，进一步抓取到章节内容 
    4.扫描图片 
    扫描出章节内容中图片资源，并把图片资源抓取到本地 
    5.手动识别图片文字 
    根据抓取下来的图片资源，识别文字 
    6.生成标准章节 
    再次扫描章节内容，替换图片，生成章节 
    7.访问本地对应链接，导出书籍txt 

后续想法 
    目前都是通过命令行操作的，感觉还是比较繁琐。手动识别文字也比较难受，所有后续的想法是弄一个可视化管理后台，方便操作，最重要的是方便处理图片文字的识别，需要解决的技术难点是如何保证网页调起命令行，以及脚本运行进度的可视化，脚本的稳定性，多线程爬虫的实现，这些都是下个版本需要考虑的。 

2021.5.10
    好久都没写总结了。目前书籍爬取的可视化管理后台已经写了个大概出来了。有些地方用着还比较僵硬，后面会继续改进。关于网页调起命令行研究了一下，目前网络找到的方法前篇一律都是通过刷新缓存区来达到边执行边输出的效果，试了下可能是因为php版本跌代的问题，所以这个方法不行。目前自己想了一个方案：前端与后端建立scoket，通过队列异步执行爬虫，爬取进度通过scoket广播回前端。这个思路应该是没有问题的，就是建立起来有点复杂，打算下一步搞这个。目前爬虫以迭代多次,稳定性还是将就,重爬机制，数据校验机制也都有了。关于图片，我觉得管理网站的已经注意到我了，老是更换图片，识别还是没有找到比较好的方法，后面想到图片应该是php生成的，相同尺寸下的文件应该是一样大小的，所以通过校验图片的md5值，来过滤重复字符，这个机制虽然还是要手动建立字典，但是降低了90%以上的重复操作，勉强能接受吧。


   除了小说的爬取，后面又想到了图片的爬取，找了几个有小姐姐图片的网站进行采集，大概采集了有50多个G的图片(嘿嘿),总体来说跟小说爬取流程差不多，先扫描目录，在爬取图片，大差不差的。

   本来是想写个博客的，最后莫名其妙的搞成了爬虫，emmmm，果然ghs是第一生产力。因为vps的ip老是被封，所以还是打算完成博客，挂在vps上装装样子，降低vps被封的几率。

   也没什么想写的了，也不知道下次什么时候在更新。
    

